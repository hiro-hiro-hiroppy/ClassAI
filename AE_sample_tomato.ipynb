{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### データセットの場所やバッチサイズなどの定数値の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 使用するデバイス\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# エポック数\n",
    "N_EPOCHS = 20\n",
    "\n",
    "# 学習時のバッチサイズ\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "DATASET_CSV = './fruits-360/train_list.csv'\n",
    "\n",
    "# 画像ファイルの先頭に付加する文字列\n",
    "DATA_DIR  = './fruits-360/'\n",
    "\n",
    "# 画像サイズ\n",
    "H = 100 # 縦幅\n",
    "W = 100 # 横幅\n",
    "C = 3 # チャンネル数（カラー画像なら3，グレースケール画像なら1）\n",
    "\n",
    "# 特徴ベクトルの次元数\n",
    "N = 128\n",
    "\n",
    "# 学習結果の保存先フォルダ\n",
    "MODEL_DIR = './AE_models_Banana/'\n",
    "\n",
    "# 学習結果のニューラルネットワークの保存先\n",
    "MODEL_FILE_ENC = os.path.join(MODEL_DIR, 'banana_encoder_model.pth') # エンコーダ\n",
    "MODEL_FILE_DEC = os.path.join(MODEL_DIR, 'banana_decoder_model.pth') # デコーダ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ニューラルネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# バナナの画像をN次元特徴ベクトルへと圧縮するニューラルネットワーク\n",
    "class BananaEncoder(nn.Module):\n",
    "    \n",
    "    # C: 入力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 入力顔画像の縦幅（8の倍数と仮定）\n",
    "    # W: 入力顔画像の横幅（8の倍数と仮定）\n",
    "    # N: 出力の特徴ベクトルの次元数\n",
    "    def __init__(self, C, H, W, N):\n",
    "        super(BananaEncoder, self).__init__()\n",
    "        \n",
    "        # カーネルサイズ（）\n",
    "        self.conv1 = nn.Conv2d(in_channels=C, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        # バッチ正規化層\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "        \n",
    "        # 平坦化\n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        # 全結合層1\n",
    "        self.fc1 = nn.Linear(in_features=H*W, out_features=2048)\n",
    "        \n",
    "        # 全結合層2\n",
    "        self.fc2 = nn.Linear(in_features=2048, out_features=N)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        h = F.leaky_relu(self.bn2(self.conv2(h)))\n",
    "        h = F.leaky_relu(self.bn3(self.conv3(h)))\n",
    "        # h = F.leaky_relu(self.bn4(self.conv4(h)))\n",
    "        #h = self.conv4(h) # 例: Residual Block を使用する場合はこのように記載（Residual Blockの内部でバッチ正規化と活性化関数を適用しているので，外側では適用しない）\n",
    "        h = self.flat(h)\n",
    "        h = F.leaky_relu(self.fc1(h))\n",
    "        z = self.fc2(h)\n",
    "        return z\n",
    "        \n",
    "# N 次元の特徴ベクトルから顔画像を生成するニューラルネットワーク\n",
    "# AutoEncoderのデコーダ部分のサンプル\n",
    "class BananaDecoder(nn.Module):\n",
    "\n",
    "    # C: 出力顔画像のチャンネル数（1または3と仮定）\n",
    "    # H: 出力顔画像の縦幅（8の倍数と仮定）\n",
    "    # W: 出力顔画像の横幅（8の倍数と仮定）\n",
    "    # N: 入力の特徴ベクトルの次元数\n",
    "    def __init__(self, C, H, W, N):\n",
    "        super(BananaDecoder, self).__init__()\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "\n",
    "        # 全結合層1,2\n",
    "        # パーセプトロン数は FaceEncoder の全結合層と真逆に設定\n",
    "        self.fc2 = nn.Linear(in_features=N, out_features=2048)\n",
    "        self.fc1 = nn.Linear(in_features=2048, out_features=H*W)\n",
    "\n",
    "        # 逆畳込み層1～4\n",
    "        # カーネルサイズ，ストライド幅，パディングは FaceEncoder の畳込み層1～4と真逆に設定\n",
    "        # self.deconv4 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        # バッチ正規化層\n",
    "        # self.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=8)\n",
    "\n",
    "        # 畳込み層\n",
    "        # 逆畳込み層の出力には checker board artifact というノイズが乗りやすいので，最後に畳込み層を通しておく\n",
    "        self.conv = nn.Conv2d(in_channels=8, out_channels=C, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.leaky_relu(self.fc2(z))\n",
    "        h = F.leaky_relu(self.fc1(h))\n",
    "        h = torch.reshape(h, (len(h), 64, self.H//8, self.W//8)) # 一列に並んだパーセプトロンを 64*(H/8)*(W/8) の特徴マップに並べ直す\n",
    "        # h = F.leaky_relu(self.bn4(self.deconv4(h)))\n",
    "        h = F.leaky_relu(self.bn3(self.deconv3(h)))\n",
    "        h = F.leaky_relu(self.bn2(self.deconv2(h)))\n",
    "        h = F.leaky_relu(self.bn1(self.deconv1(h)))\n",
    "        y = torch.sigmoid(self.conv(h))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 訓練データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 指定されたプロシージャが見つかりません。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from mylib.data_io import CSVBasedDataset\n",
    "from mylib.utility import save_datasets, load_datasets_from_file\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# 再開モードの場合は，前回使用したデータセットをロードして使用する\n",
    "if RESTART_MODE:\n",
    "    train_dataset, valid_dataset = load_datasets_from_file(MODEL_DIR)\n",
    "    if train_dataset is None:\n",
    "        print('error: there is no checkpoint previously saved.')\n",
    "        exit()\n",
    "    train_size = len(train_dataset)\n",
    "    valid_size = len(valid_dataset)\n",
    "\n",
    "# そうでない場合は，データセットを読み込む\n",
    "else:\n",
    "\n",
    "    # CSVファイルを読み込み, 訓練データセットを用意\n",
    "    dataset = CSVBasedDataset(\n",
    "        filename = DATASET_CSV,\n",
    "        items = [\n",
    "            'File Path', # X\n",
    "        ],\n",
    "        dtypes = [\n",
    "            'image', # Xの型\n",
    "        ],\n",
    "        dirname = DATA_DIR,\n",
    "        img_transform = transforms.CenterCrop((H, W)), # 処理量を少しでも抑えるため，画像中央の H×W ピクセルの部分だけを対象とする\n",
    "    )  \n",
    "    \n",
    "    # 訓練データセットを分割し，一方を検証用に回す\n",
    "    dataset_size = len(dataset)\n",
    "    valid_size = int(0.002 * dataset_size) # 全体の 0.2% を検証用に -> tinyCelebA の画像は全部で 16000 枚なので，検証用画像は 16000*0.002=32 枚\n",
    "    train_size = dataset_size - valid_size # 残りの 99.8% を学習用に\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "    # データセット情報をファイルに保存\n",
    "    save_datasets(MODEL_DIR, train_dataset, valid_dataset)\n",
    "\n",
    "# 訓練データおよび検証用データをミニバッチに分けて使用するための「データローダ」を用意\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'image' object has no attribute 'read_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m sum_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# for X in tqdm(train_dataloader):\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m enc_model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m     50\u001b[0m         param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\Git\\ClassAI\\mylib\\data_io.py:159\u001b[0m, in \u001b[0;36mCSVBasedDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)):\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;66;03m# 実際に画像ファイルを読み込み，画素値を正規化する\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m         x \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_image(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[i][index]), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_mode)\n\u001b[0;32m    160\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_range_coeff \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_range[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torchvision\\io\\image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m--> 258\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[1;32mc:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torchvision\\io\\image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[1;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mread_file(path)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\HIROYUKI\\anaconda3\\envs\\envA\\Lib\\site-packages\\torch\\_ops.py:921\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[1;34m(self, op_name)\u001b[0m\n\u001b[0;32m    919\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_get_operation(qualified_op_name)\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    922\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    923\u001b[0m         )\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    928\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    929\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_OpNamespace' 'image' object has no attribute 'read_file'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from mylib.visualizers import LossVisualizer\n",
    "from mylib.data_io import show_images\n",
    "from mylib.utility import save_checkpoint, load_checkpoint\n",
    "\n",
    "\n",
    "# 前回の試行の続きを行いたい場合は True にする -> 再開モードになる\n",
    "RESTART_MODE = False\n",
    "\n",
    "\n",
    "# エポック番号\n",
    "INIT_EPOCH = 0 # 初期値\n",
    "LAST_EPOCH = INIT_EPOCH + N_EPOCHS # 最終値\n",
    "\n",
    "# ニューラルネットワークの作成\n",
    "enc_model = BananaEncoder(C=C, H=H, W=W, N=N).to(DEVICE)\n",
    "dec_model = BananaDecoder(C=C, H=H, W=W, N=N).to(DEVICE)\n",
    "\n",
    "# 最適化アルゴリズムの指定（ここでは SGD でなく Adam を使用）\n",
    "enc_optimizer = optim.Adam(enc_model.parameters())\n",
    "dec_optimizer = optim.Adam(dec_model.parameters())\n",
    "\n",
    "# 再開モードの場合は，前回チェックポイントから情報をロードして学習再開\n",
    "# if RESTART_MODE:\n",
    "#     INIT_EPOCH, LAST_EPOCH, enc_model, enc_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_ENC_MODEL, CHECKPOINT_ENC_OPT, N_EPOCHS, enc_model, enc_optimizer)\n",
    "#     _, _, dec_model, dec_optimizer = load_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DEC_MODEL, CHECKPOINT_DEC_OPT, N_EPOCHS, dec_model, dec_optimizer)\n",
    "#     print('')\n",
    "\n",
    "# 損失関数\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# 損失関数値を記録する準備\n",
    "loss_viz = LossVisualizer(['train loss', 'valid loss'], init_epoch=INIT_EPOCH)\n",
    "\n",
    "# 勾配降下法による繰り返し学習\n",
    "for epoch in range(INIT_EPOCH, LAST_EPOCH):\n",
    "\n",
    "    print('Epoch {0}:'.format(epoch + 1))\n",
    "\n",
    "    # 学習\n",
    "    enc_model.train()\n",
    "    dec_model.train()\n",
    "    sum_loss = 0\n",
    "    # for X in tqdm(train_dataloader):\n",
    "    for X in train_dataloader:\n",
    "        for param in enc_model.parameters():\n",
    "            param.grad = None\n",
    "        for param in dec_model.parameters():\n",
    "            param.grad = None\n",
    "        X = X.to(DEVICE)\n",
    "        Z = enc_model(X) # 入力画像 X を現在のエンコーダに入力し，特徴ベクトル Z を得る\n",
    "        Y = dec_model(Z) # 特徴ベクトル Z を現在のデコーダに入力し，復元画像 Y を得る\n",
    "        loss = loss_func(X, Y) # 損失関数の現在値を計算\n",
    "        loss.backward() # 誤差逆伝播法により，個々のパラメータに関する損失関数の勾配（偏微分）を計算\n",
    "        enc_optimizer.step() # 勾配に沿ってパラメータの値を更新\n",
    "        dec_optimizer.step() # 同上\n",
    "        sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / train_size\n",
    "    loss_viz.add_value('train loss', avg_loss) # 訓練データに対する損失関数の値を記録\n",
    "    print('train loss = {0:.6f}'.format(avg_loss))\n",
    "\n",
    "    # 検証\n",
    "    enc_model.eval()\n",
    "    dec_model.eval()\n",
    "    sum_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        # for X in tqdm(valid_dataloader):\n",
    "        for X in valid_dataloader:\n",
    "            X = X.to(DEVICE)\n",
    "            Z = enc_model(X)\n",
    "            Y = dec_model(Z) \n",
    "            loss = loss_func(X, Y)\n",
    "            sum_loss += float(loss) * len(X)\n",
    "    avg_loss = sum_loss / valid_size\n",
    "    loss_viz.add_value('valid loss', avg_loss) # 検証用データに対する損失関数の値を記録\n",
    "    print('valid loss = {0:.6f}'.format(avg_loss))\n",
    "    print('')\n",
    "\n",
    "    # 学習経過の表示\n",
    "    if epoch == 0:\n",
    "        show_images(X.to('cpu').detach(), num=BATCH_SIZE, num_per_row=8, title='original', save_fig=False, save_dir=MODEL_DIR)\n",
    "    show_images(Y.to('cpu').detach(), num=BATCH_SIZE, num_per_row=8, title='epoch {0}'.format(epoch + 1), save_fig=False, save_dir=MODEL_DIR)\n",
    "\n",
    "    # # 現在の学習状態を一時ファイル（チェックポイント）に保存\n",
    "    # save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_ENC_MODEL, CHECKPOINT_ENC_OPT, epoch+1, enc_model, enc_optimizer)\n",
    "    # save_checkpoint(CHECKPOINT_EPOCH, CHECKPOINT_DEC_MODEL, CHECKPOINT_DEC_OPT, epoch+1, dec_model, dec_optimizer)\n",
    "\n",
    "# 学習結果のニューラルネットワークモデルをファイルに保存\n",
    "enc_model = enc_model.to('cpu')\n",
    "dec_model = dec_model.to('cpu')\n",
    "torch.save(enc_model.state_dict(), MODEL_FILE_ENC)\n",
    "torch.save(dec_model.state_dict(), MODEL_FILE_DEC)\n",
    "\n",
    "# 損失関数の記録をファイルに保存\n",
    "loss_viz.save(v_file=os.path.join(MODEL_DIR, 'loss_graph.png'), h_file=os.path.join(MODEL_DIR, 'loss_history.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
